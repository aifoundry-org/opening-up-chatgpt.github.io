project.link,project.notes,project.llmbase,project.rlbase,project.license,org.name,org.link,org.notes,opencode.class,opencode.link,opencode.notes,llmdata.class,llmdata.link,llmdata.notes,llmweights.class,llmweights.link,llmweights.notes,rldata.class,rldata.link,rldata.notes,rlweights.class,rlweights.link,rlweights.notes,license.class,license.link,license.notes,code.class,code.link,code.notes,architecture.class,architecture.link,architecture.notes,preprint.class,preprint.link,preprint.notes,paper.class,paper.link,paper.notes,modelcard.class,modelcard.link,modelcard.notes,datasheet.class,datasheet.link,datasheet.notes,package.class,package.link,package.notes,api.class,api.link,api.notes,source.file,openness
https://blog.allenai.org/olmo-open-language-model-87ccfc95f580,,OLMo 7B,OpenInstruct,Apache 2.0,Ai2,https://allenai.org,Allen Institute for AI (non-profit research institute),open,https://github.com/allenai/OLMo,"Multiple repos with training, architecture and fine-tuning code available",open,https://huggingface.co/datasets/allenai/dolma,Dolma training data released and documented in exemplary way,open,https://huggingface.co/collections/allenai/olmo-suite-65aeaae8fe5b6b2122b46778,OLMo 7B and many training checkpoints available,open,https://huggingface.co/datasets/allenai/ultrafeedback_binarized_cleaned,Instruction tuning datasets documented and made available in exemplary ways,open,https://huggingface.co/allenai/OLMo-7B-Instruct/tree/main,Full model weights made available,open,https://huggingface.co/allenai/OLMo-7B-Instruct,Apache 2.0,open,https://huggingface.co/allenai/OLMo-7B-Instruct#model-sources,"repositories and code well-described, commented and documented",open,https://huggingface.co/allenai/OLMo-7B-Instruct#model-sources,"Architectured documented in detail in model card, preprint, and technical blog posts",open,https://arxiv.org/abs/2402.00838,"Preprint describes model architecture, training and fine-tuning data, and training and SFT pipelines",closed,,No peer-reviewed paper found,open,https://huggingface.co/allenai/OLMo-7B-Instruct,Model card provides broad overview and links to full details,open,https://huggingface.co/datasets/allenai/dolma,"Data sheets and documentation available for the datasets used, linked here is Dolma",open,https://pypi.org/project/ai2-olmo/,AI2-olmo available on PyPi,partial,https://huggingface.co/allenai/OLMo-7B-hf,Available through HuggingFace though model is too large to run on free inference API,/projects/olmo-7b-instruct.yaml,12.5
https://huggingface.co/bigscience/bloomz,,"BLOOMZ, mT0",xP3,Apache 2.0 and RAIL (responsible AI license),bigscience-workshop,https://github.com/bigscience-workshop,,open,https://github.com/bigscience-workshop/xmtf,Repository provides a guided overview to all components,open,https://github.com/bigscience-workshop/xmtf#data,Data made available & documented in detail in repo and preprint,open,https://github.com/bigscience-workshop/xmtf#models,Model made available on github,open,https://huggingface.co/datasets/bigscience/xP3all,From the documentation 'xP3 (Crosslingual Public Pool of Prompts) is a collection of prompts & datasets across 46 of languages & 16 NLP tasks',partial,https://huggingface.co/bigscience/bloomz-optimizer-states/tree/main,Fine-tuned checkpoint available for download,partial,https://bigscience.huggingface.co/blog/the-bigscience-rail-license,"Code licensed under Apache 2.0, model under bespoke 'Responsible AI License' which imposes some limitations",open,https://github.com/bigscience-workshop/xmtf,Code well documented and actively maintained,open,https://github.com/bigscience-workshop/xmtf#create-xp3x,"Architecture described in preprint, code available in github repo, recipe on HuggingFace",open,https://arxiv.org/abs/2211.05100,Preprint (updated June 2023) of 65 pages + 10 page appendix,open,https://aclanthology.org/2023.acl-long.891/,Peer-reviewed paper of 9 pages + 114 page appendix describes the multitask finetuning (instruction tuning) of BLOOM (see preprint) to form BLOOMZ,open,https://huggingface.co/bigscience/bloomz,Model card,open,https://huggingface.co/datasets/bigscience/xP3,Dataset documented in dataset card at HuggingFace,closed,,No packages published,open,https://huggingface.co/spaces/bigscience/petals-api,Petals API via HuggingFace not always available ('not enough hardware capacity'),/projects/bloomz.yaml,12.0
https://huggingface.co/LLM360/AmberChat,,Amber,ShareGPT + Evol-Instruct (synthetic),Apache 2.0,LLM360,https://www.llm360.ai/index.html,"LLM360, an initiative to fully open-source LLMs",open,https://github.com/LLM360/amber-train/tree/main,amber-train repository includes code for training and finetuning.,open,https://huggingface.co/datasets/LLM360/AmberDatasets,data well-documented and openly available,open,https://huggingface.co/LLM360/Amber,360 model checkpoints released,open,https://huggingface.co/datasets/WizardLM/WizardLM_evol_instruct_V2_196k,RL and fine-tuning data shared and documented,open,https://huggingface.co/LLM360/AmberChat,Finetuned model available for download.,open,https://huggingface.co/LLM360/AmberChat,Everything licensed under Apache 2.0,partial,https://github.com/LLM360,Code documented in helpful readme.md files but only partly inline.,partial,https://arxiv.org/abs/2312.06550,"Architecture described in preprint, but not all details documented.",open,https://arxiv.org/abs/2312.06550,"Preprint describes architecture, design choices, training and fine-tuning.",closed,,No peer-reviewed paper yet.,partial,https://huggingface.co/LLM360/AmberChat,Model card doesn't specify use or limitations,partial,https://huggingface.co/datasets/LLM360/AmberDatasets,"Concise description (better than most), but doesn't specify funders, purposes, representativeness, legal status as prescribed by datasheets industry standard",closed,,No released package found,open,https://huggingface.co/LLM360/AmberChat,Free Huggingface inference API.,/projects/amber.yaml,10.0
https://open-assistant.io/,,Pythia 12B,OpenAssistant Conversations,Apache 2.0,LAION-AI,https://open-assistant.io/,,open,https://github.com/LAION-AI/Open-Assistant,Code includes guide for developers,open,https://github.com/LAION-AI/Open-Assistant/tree/main/data/datasets,Datasets documented in detail and recipes for cleaning up and downloading provided in code notebooks.,open,https://huggingface.co/OpenAssistant,Model weights in several variants downloadable through HuggingFace,open,https://huggingface.co/datasets/OpenAssistant/oasst1,"OpenAssistant Conversations is 'a human-generated, human-annotated assistant-style conversation corpus consisting of 161443 messages distributed across 66497 conversation trees, in 35 different languages, annotated with 461292 quality ratings' (preprint)",closed,,RLHF weights not separately released,open,https://projects.laion.ai/Open-Assistant/docs/faq#what-license-does-open-assistant-use,Apache 2.0,open,https://projects.laion.ai/Open-Assistant/docs/intro,Separate website provides entry point to comprehensive documentation,open,https://github.com/LAION-AI/Open-Assistant/tree/main/model,Instructions to tune the pipeline on training data,partial,https://arxiv.org/abs//2304.07327,"Preprint describes creation of OpenAssistant Conversations corpus for instruction tuning, but not the base LLM, hence partial.",closed,,No peer-reviewed paper or published data audit found,closed,,,closed,,,open,,,open,https://projects.laion.ai/Open-Assistant/api,,/projects/Open-Assistant.yaml,9.5
https://github.com/imoneoi/openchat,,Mistral 7B,ShareGPT with C-RLFT,Apache 2.0,Tshinghua University,https://github.com/imoneoi,OpenChat notes 'We are a student team from Tsinghua University',open,https://github.com/imoneoi/openchat/tree/master/ochat,Repository offers a large amount of fairly well-organized code for data curation and model,closed,,Pretraining data for Mistral is nowhere disclosed or documented,open,https://github.com/mistralai/mistral-src#download-the-model,Mistral 7B weights available via Mistral repository,closed,,Preprint says shareGPT dataset 'collected from sharegpt.com' but not disclosed or made available by this project,open,https://huggingface.co/openchat/openchat_3.5/tree/main,Instruction-tuned model weights made available via HuggingFace,open,https://github.com/imoneoi/openchat/blob/master/LICENSE,Code and model released under Apache 2.0,partial,https://github.com/imoneoi/openchat/tree/master/ochat,There is plenty of code in the github repository but only some of it is documented,open,https://arxiv.org/abs/2309.11235,Architecture quite well described in preprint,open,https://arxiv.org/abs/2309.11235,"Preprint describes the model architecture and instruction tuning approach, though is hampered by building on notoriously closed Llama2",open,https://openreview.net/forum?id=AOJyfhWYHf,Paper reviewed and accepted for ICLR 2024,partial,https://huggingface.co/openchat/openchat_v3.2,There is a model card that provides some details on architecture and evaluation,closed,,Datasheet not provided.,open,https://github.com/imoneoi/openchat/tree/master#installation,Python package 'ochat' provided through pip,partial,,"Model too large to load onto HuggingFace free inference API, so only available through Inference Endpoints or package",/projects/OpenChat.yaml,9.5
https://huggingface.co/togethercomputer/Pythia-Chat-Base-7B,,EleutherAI pythia,OIG,Apache 2.0 license,togethercomputer,https://github.com/togethercomputer,,open,,,open,https://github.com/togethercomputer/OpenDataHub,Training data curated and shared in separate repository,open,https://huggingface.co/togethercomputer/Pythia-Chat-Base-7B/tree/main,Model weights available via HuggingFace,open,https://huggingface.co/datasets/laion/OIG,From the documentation 'This is our attempt to create a large instruction dataset of medium quality along with a smaller high quality instruciton dataset (OIG-small-chip2).',closed,,RL weights not separately made available,open,https://huggingface.co/togethercomputer/Pythia-Chat-Base-7B#model-details,Apache 2.0,open,https://github.com/togethercomputer/OpenChatKit,Actively maintained repository,open,https://github.com/togethercomputer/OpenChatKit#reproducing-pythia-chat-base-7b,Architecture and recipe for reproducing model provided,partial,https://arxiv.org/abs/2304.01373,Preprint describes LM base (Pythia) but not instruction tuning details,closed,,No peer-reviewed paper or data audit found,partial,https://huggingface.co/togethercomputer/Pythia-Chat-Base-7B,Model card partially available but fairly minimally specified,partial,https://huggingface.co/datasets/laion/OIG,OIG instruction dataset documented,open,,,closed,,,/projects/pythia-chat-base-7B.yaml,9.5
https://huggingface.co/SebastianSchramm/Cerebras-GPT-111M-instruction,,Cerebras,Alpaca (synthetic),Apache 2.0,Cerebras + Schramm,https://github.com/Cerebras,,partial,https://github.com/Cerebras/gigaGPT,"Some of the training code available in GigaGPT, but fine-tuning",open,https://huggingface.co/datasets/EleutherAI/pile,Eleuther AI's The Pile,open,https://huggingface.co/cerebras/Cerebras-GPT-111M,base model available via Cerebras,open,https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM/blob/main/data/alpaca_gpt4_data.json,Alpaca GPT4,open,https://huggingface.co/SebastianSchramm/Cerebras-GPT-111M-instruction/tree/main,Finetuned model weights available,partial,https://huggingface.co/SebastianSchramm/Cerebras-GPT-111M-instruction/,Licensing situation unclear as model page mentions no license (base model is licensed Apache 2.0),closed,,Code only sparsely documented,open,,Described in preprint,partial,https://arxiv.org/abs/2304.03208,,closed,,,closed,https://huggingface.co/cerebras/Cerebras-GPT-111M,Only serves as as advertising for the model,open,https://arxiv.org/abs/2201.07311,Datasheet available for The Pile,closed,,No package found,open,,Available through HuggingFace inference API,/projects/cerebras-gpt-111m-instruction.yaml,8.5
https://huggingface.co/togethercomputer/RedPajama-INCITE-7B-Instruct,,RedPajama-INCITE-7B-Base,various (GPT-JT recipe),Apache 2.0,TogetherComputer,https://together.ai/,,partial,https://github.com/togethercomputer/redpajama.cpp/tree/master/examples/redpajama,Code for datasets made available in exemplary ways; code for training and tuning harder to find,open,https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T,RedPajama-Data-1T made available on HuggingFace,open,https://huggingface.co/togethercomputer/RedPajama-INCITE-7B-Base,Base is RedPajama-INCITE-7B-Base,open,https://huggingface.co/datasets/togethercomputer/RedPajama-Data-Instruct,"The model was trained on a large collection of diverse data, including Chain-of-Thought (CoT), Public Pool of Prompts (P3) dataset, Natural-Instructions (NI) dataset.",open,https://huggingface.co/togethercomputer/RedPajama-INCITE-7B-Instruct,Instruction-tuned version made available in paralle with base version,partial,https://huggingface.co/togethercomputer/RedPajama-INCITE-7B-Instruct/blob/main/README.md,"Models licensed under Apache 2.0, but note that the data itself is variably licensed and so imposes some limitations.",partial,,Code for base LLM and instruction tuning datasets beautifully documented; code specifying training and fine-tuning sparsely documented.,partial,https://together.ai/blog/redpajama,"Architecture detailed on model card, crucial parts appear to be forked from GPT-NeoX",closed,,No preprint found,closed,,No paper found,open,https://huggingface.co/togethercomputer/RedPajama-INCITE-7B-Instruct,Model card and readme provide details on datasets and,open,https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T,Data sheet includes links to data and recipes to create from scratch,closed,,No separate package found,partial,https://huggingface.co/togethercomputer/RedPajama-INCITE-7B-Instruct,Hosted inference API available through HuggingFace,/projects/RedPajama-INCITE-Instruct-7B.yaml,8.5
https://github.com/databrickslabs/dolly,,EleutherAI pythia,databricks-dolly-15k,,databricks,https://www.databricks.com,,open,,,open,,,open,,,open,,,closed,,,open,,,open,,,open,,,partial,https://arxiv.org/abs/2304.01373,,closed,,,closed,,,closed,,,open,,,closed,,,/projects/dolly.yaml,8.5
https://huggingface.co/allenai/tulu-2-dpo-70b,A new DPO model from AllenAI called Tülu,Llama2,"Tulu SFT, Ultrafeedback",AI2 ImpACT license,AllenAI,https://allenai.org/,,open,https://github.com/allenai/open-instruct,Important effort to make available fine-tuning procedure and source code. Not seen a repository for base model training yet.,closed,,Based on Llama2 so nothing known about training data,partial,https://github.com/meta-llama/,Base model made available via Meta (requires privacy-defying signup),open,https://github.com/allenai/open-instruct,Codebase used for fine-tuning this model and made available for others,open,https://huggingface.co/allenai/tulu-2-dpo-70b/tree/main,Fine-tuned weights available,partial,https://allenai.org/impact-license,Meta Community License and AI2 Impact license,partial,https://github.com/allenai/open-instruct,OpenInstruct code well-documented,partial,https://huggingface.co/HuggingFaceH4/zephyr-7b-beta,"Post-llama2 aspects quite well described in papers, e.g. DPO based on Zephyr Beta, but base model is irrevocably closed",open,https://arxiv.org/abs/2311.10702,Preprint covers the training of Tulu2,closed,,No peer-reviewed work found,partial,https://huggingface.co/allenai/tulu-2-70b,Model card offers little details,partial,https://huggingface.co/datasets/allenai/tulu-v2-sft-mixture,"Some datasheets available (for instruction tuning), but base model entirely undocumented.",closed,,No separate package found,open,,Available via various APIs,/projects/tulu-2-70b.yaml,8.0
https://huggingface.co/mosaicml/mpt-30b-instruct,,MosaicML,"dolly, anthropic",CC-By-SA-3.0,MosaicML,https://www.mosaicml.com,,open,https://github.com/mosaicml/llm-foundry/tree/main/llmfoundry/models/mpt,Codebase part of LLM foundry,partial,https://huggingface.co/datasets/c4,C4 is part of the dataset but a precise specification of source data is hard to find,open,https://huggingface.co/mosaicml/mpt-30b-instruct/tree/main,Weights available via HuggingFace,partial,https://huggingface.co/datasets/mosaicml/dolly_hhrlhf,"dolly-hhrlhf, combination of Databrick dolly-15k dataset and a filtered subset of Anthropic HH-RLHF",closed,,,open,,CC-by-SA 3.0,open,https://github.com/mosaicml/llm-foundry/,LLM Foundry codebase is well-documented and in active development.,partial,https://huggingface.co/mosaicml/mpt-30b-instruct,Architecture reasonably well-documented,closed,,,closed,,,partial,https://huggingface.co/mosaicml/mpt-30b-instruct,Modelcard is somewhat lacking in detail,closed,https://www.mosaicml.com/blog/mpt-30b,Datasheet not available; data somewhat documented in blog post at link,open,,,partial,,API via HuggingFace,/projects/MPT-30b-instruct.yaml,7.5
https://huggingface.co/mosaicml/mpt-7b-instruct,,MosaicML,"dolly, anthropic",CC-By-SA-3.0,MosaicML,https://www.mosaicml.com,,open,https://github.com/mosaicml/llm-foundry/tree/main/llmfoundry/models/mpt,Codebase part of LLM foundry,partial,https://huggingface.co/datasets/c4,C4 is part of the dataset but a precise specification of source data is hard to find,open,https://huggingface.co/mosaicml/mpt-7b-instruct/tree/main,Weights available via HuggingFace,partial,https://huggingface.co/datasets/mosaicml/dolly_hhrlhf,"dolly-hhrlhf, combination of Databrick dolly-15k dataset and a filtered subset of Anthropic HH-RLHF",closed,,,open,,CC-by-SA 3.0,open,https://github.com/mosaicml/llm-foundry/,LLM Foundry codebase is well-documented and in active development,partial,https://huggingface.co/mosaicml/mpt-7b-instruct,Architecture reasonably well-documented,closed,,,closed,,,open,https://huggingface.co/mosaicml/mpt-7b-instruct,,closed,,,open,,,closed,,,/projects/MPT-7b-instruct.yaml,7.5
https://github.com/CarperAI/trlx,,"various (pythia, flan, OPT)",various,MIT license,carperai,https://github.com/CarperAI/trlx,,open,,,open,,,open,,,partial,,,closed,,,open,,,open,,,partial,,,closed,,,closed,,,closed,,,closed,,,partial,,,open,,,/projects/trlx.yaml,7.5
https://huggingface.co/Intel/neural-chat-7b-v3-1,A mistral-based Orca-finetuned chat model,Mistral 7B,Orca,Apache 2.0,Intel,https://community.intel.com/t5/Blogs/Tech-Innovation/Artificial-Intelligence-AI/NeuralChat-A-Customizable-Chatbot-Framework/post/1526789,,partial,https://github.com/intel/intel-extension-for-transformers/tree/main/intel_extension_for_transformers/neural_chat/examples/finetuning/finetune_neuralchat_v3,"Mistral base model is not open, but repo gives sample code for fine-tuning based on that",closed,,Mistral has not disclosed anything about its training data,open,https://huggingface.co/mistralai/Mistral-7B-v0.1,Based on Mistral 7B 0.1,open,https://huggingface.co/datasets/Open-Orca/SlimOrca,RL dataset used for post-training is shared and available,open,https://huggingface.co/Intel/neural-chat-7b-v3-1/tree/main,finetuned model made openly available by Intel,open,https://huggingface.co/Intel/neural-chat-7b-v3-1/blob/main/LICENSE,Apache 2.0,partial,https://github.com/intel/intel-extension-for-transformers/tree/main/intel_extension_for_transformers/neural_chat/examples/finetuning/finetune_neuralchat_v3#how-to-train-intelneural-chat-7b-v3-1-on-intel-gaudi2,"Mistral remains closed so only documentation pertains to fine-tuning steps, but that code is quite well documented, so partial.",partial,https://medium.com/intel-analytics-software/the-practice-of-supervised-finetuning-and-direct-preference-optimization-on-habana-gaudi2-a1197d8a3cd3,Described in on HuggingFace model card and a Medium post,closed,https://medium.com/intel-analytics-software/the-practice-of-supervised-finetuning-and-direct-preference-optimization-on-habana-gaudi2-a1197d8a3cd3,A medium post is apparently the only scientific documentation of this model,closed,,No peer-reviewed paper found,partial,https://huggingface.co/Intel/neural-chat-7b-v3-1,No model card for Mistral base model portion.,partial,https://huggingface.co/datasets/Open-Orca/OpenOrca,SlimOrca dataset is described as part of OpenOrca (but partial since Mistral data is incrutable),partial,https://huggingface.co/Intel/neural-chat-7b-v3-1#fp32-inference-with-transformers,Code for running with transformers provided by Intel,closed,,"Provided through HuggingFace but model too large to run via inference API, local deployment or paid access needed",/projects/neuralchat-7b.yaml,7.0
https://huggingface.co/lmsys/vicuna-13b-v1.3,Vicuna is a chat assistant trained by fine-tuning LLaMA on user-shared conversations collected from ShareGPT.,LLaMA,ShareGPT,Non-commercial license,LMSYS,https://lmsys.org/,"According to its website, 'The Large Model Systems Organisation develops large models and systems that are open, accessible and scalable'",open,https://github.com/lm-sys/FastChat,Actively maintained repository,partial,https://github.com/facebookresearch/llama/blob/main/MODEL_CARD.md#training-dataset,"Vicuna is fine-tuned LLaMA, and LLaMA in turn is based on 'publicly available datasets' that are not all specified or easily downloadable.",open,https://github.com/lm-sys/FastChat#vicuna-weights,"Unlike Vicuna 13B v0, these weights do not require applying delta",closed,https://github.com/lm-sys/FastChat#fine-tuning,From the documentation 'We will not release the ShareGPT dataset'. Also 'Vicuna v1.3 is fine-tuned from LLaMA with supervised instruction fine-tuning. The training data is around 140K conversations collected from ShareGPT.com.',closed,https://github.com/lm-sys/FastChat#fine-tuning,No model weights are shared for the instruction tuning,partial,https://github.com/lm-sys/FastChat#vicuna-weights,From the documentation 'Vicuna is based on LLaMA and should be used under LLaMA's model license.',open,https://github.com/lm-sys/FastChat,Code is quite well-documented and released as part of the FastChat framework.,closed,,,open,https://arxiv.org/pdf/2306.05685.pdf,Preprint covers training of the Vicuna model.,closed,,No peer-reviewed paper.,partial,https://huggingface.co/lmsys/vicuna-13b-v1.3,"Minimal model card, but many details are not provided or have to be pieced together from elsewhere.",closed,,No datasheet provided.,open,https://pypi.org/project/fschat/0.1.2/,Available via pip,partial,https://github.com/lm-sys/FastChat#api,"Support provided for several APIs OpenAI restful, HuggingFace, Langchain",/projects/vicuna13B-lmsys.yaml,7.0
hhttps://github.com/ethanyanjiali/minChatGPT,,GPT2,anthropic,GNU General Public License v3.0,ethanyanjiali,https://github.com/ethanyanjiali/minChatGPT,,open,,,open,,,open,,,partial,,,closed,,,open,,,open,,,partial,,,closed,,,closed,,,closed,,,closed,,,closed,,,open,,,/projects/minChatGPT.yaml,7.0
https://github.com/BlinkDL/ChatRWKV,,RWKV-LM,"alpaca, shareGPT (synthetic)",,BlinkDL/RWKV,https://www.rwkv.com/,,open,https://github.com/BlinkDL/ChatRWKV,Various community-contributed enhancements available,partial,https://pile.eleuther.ai/,Trained on The Pile. Recent versions also build on Red Pajama (https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T),open,https://huggingface.co/BlinkDL/rwkv-4-world/tree/main,Model weights released across different HuggingFace spaces,closed,,"Instruction tuning data not separately available. Documentation 'These are RWKV-4-Pile 1.5/3/7/14B models finetuned on Alpaca, CodeAlpaca, Guanaco, GPT4All, ShareGPT and more'",closed,,Weights not separately available.,open,https://github.com/BlinkDL/ChatRWKV/blob/main/LICENSE,Apache 2.0,partial,,Code documentation scattered across github repo and HuggingFace spaces,partial,,Architecture described in preprint (LM part) but not all details clearly documented.,partial,https://arxiv.org/abs/2305.13048,"Preprint covers only LLM (RNN based), not instruction fine-tuning, so partial.",closed,,No peer-reviewed paper or published data audit known,closed,https://huggingface.co/BlinkDL/rwkv-4-raven,"No modelcard, HuggingFace spaces only used to share files",closed,https://huggingface.co/BlinkDL/rwkv-4-raven,"No data sheet, HuggingFac spaces only used to share files",open,https://pypi.org/project/rwkv/,Available through pip install rwkv,partial,,API via HuggingFace,/projects/ChatRWKV.yaml,6.5
https://github.com/LianjiaTech/BELLE,,LLaMA & BLOOMZ,"alpaca, shareGPT, Belle (synthetic)",Apache License 2.0,KE Technologies,http://www.ke.com,,open,https://github.com/LianjiaTech/BELLE,Repository contains a fair bit of code,partial,,"Open for variants based on BLOOMZ. Closed for variants based on LLaMA, whose pretraining data is nowhere disclosed or documented.",partial,,LLaMA based but copyright status unclear,partial,https://github.com/LianjiaTech/BELLE/tree/main/data/1.5M,Synthetic BELLE training data in Chinese released in batches,partial,https://github.com/LianjiaTech/BELLE/tree/main/models,"Some models available, most only as delta weights requiring separate access to LLaMA",closed,,Lowest common denominator is non-OSI approved LLaMA licence agreement,partial,https://github.com/LianjiaTech/BELLE/blob/main/README_en.md,"Quite some documentation on Github, though not all well-organized",open,https://github.com/LianjiaTech/BELLE/blob/main/README_en.md,Specified in a fair bit of detail on github,open,https://arxiv.org/abs/2303.14742,,closed,,No peer-reviewed paper found,closed,,No model card found,partial,,No data sheet found,closed,,No dedicated package available,closed,,No API found,/projects/BELLE.yaml,6.0
https://huggingface.co/BramVanroy/GEITje-7B-ultra,Dutch instruction-tuned model based on Mistral 7B,Mistral 7B,Ultrafeedback Dutch (synthetic),CC-BY-ND,Bram van Roy,https://huggingface.co/BramVanroy/GEITje-7B-ultra,,closed,https://huggingface.co/BramVanroy/GEITje-7B-ultra#training-procedure,Mistral has limited source code available. No training code for Geitje found apart from the recipe used with the alignment handbook.,partial,https://huggingface.co/Rijgersberg/GEITje-7B#geitje--trained-further-on-dutch-texts,"Mistral provides no documentation of any of its pretraining data. Geitje Ultra 7B is based on Geitje 7B, which does disclose that Dutch pretraining data includes Gigacorpus and MADLAD.",open,https://github.com/mistralai/mistral-src#download-the-model,Mistral base model is available for downloading,open,https://huggingface.co/datasets/BramVanroy/ultra_feedback_dutch,Ultrafeedback Dutch (synthetic),open,https://huggingface.co/BramVanroy/GEITje-7B-ultra/tree/main,Instruction-tuned model made available through HuggingFace,closed,https://huggingface.co/BramVanroy/GEITje-7B-ultra,"Licensed as CC-BY-ND-4.0 on HuggingFace, though no specific license file or statement found",closed,,Only limited code repositories and no clear centralized documentation of code,partial,https://huggingface.co/BramVanroy/GEITje-7B-ultra,Some information on architecture provided in github repo and HF model card. Training was done using alignment handbook.,partial,https://arxiv.org/abs/2312.12852v1,Preprint documents Dutch language resources but architecture and scientific documentation otherwise lacking due to Mistral base,closed,,No peer-reviewed paper found,partial,https://huggingface.co/BramVanroy/GEITje-7B-ultra,Modelcard on HF provides information on fine-tuning but nothing for the Mistral base LLM,partial,https://huggingface.co/datasets/BramVanroy/ultra_feedback_dutch,"Datasheet available for DPO and for the Dutch portions of pretraining data, but not for original Mistral pretraining data, hence partial.",closed,,No package available,partial,,Model available through HuggingFace API,/projects/geitje-ultra.yaml,6.0
https://huggingface.co/microsoft/Phi-3-mini-128k-instruct,,Phi3,Unspecified,MIT License,Microsoft,https://huggingface.co/microsoft/Phi-3-mini-128k-instruct,,closed,,"No source code found for pretraining, posttraining, or evaluation",closed,,No datasets made available and no information on datasets disclosed except very generic claims about filtering for high quality.,closed,,No base model of the instruction-tuned Phi-3 was released,closed,,No post-training datasets made available and no information on datasets disclosed except very generic claims about filtering for high quality.,open,https://huggingface.co/microsoft/Phi-3-mini-128k-instruct/tree/main,Instruction-tuned model weights shared through HuggingFace,open,https://huggingface.co/microsoft/Phi-3-mini-128k-instruct/blob/main/LICENSE,MIT License,closed,,"No source code, so no documentation of source code found",open,https://arxiv.org/abs/2404.14219,Architecture described in model card and preprint,partial,https://arxiv.org/abs/2404.14219,"Preprint describes model architecture but not training data, focusing mostly on benchmarks and evalution",closed,,No paper found,open,https://huggingface.co/microsoft/Phi-3-mini-128k-instruct/blob/main/LICENSE,Model card provides reasonable level of detail,closed,,No datasheet made available,partial,,Available through development version of transformers,open,,Available through HuggingFace API,/projects/phi-3-instruct.yaml,6.0
https://huggingface.co/WizardLM/WizardLM-13B-V1.2,Empowering Large Pre-Trained Language Models to Follow Complex Instructions,LLaMA2-13B,Evol-Instruct (synthetic),CC-BY-NC-4.0,Microsoft & Peking University,https://github.com/nlpxucan,,partial,https://github.com/nlpxucan/WizardLM/tree/main/WizardLM,Fast-evolving repository contains WizardLM code,closed,https://github.com/opening-up-chatgpt/opening-up-chatgpt.github.io/blob/main/projects/llama-2-chat.yaml,"Based on LLaMA2, which is claimed to be public but nowhere exactly documented.",partial,https://ai.meta.com/resources/models-and-libraries/llama-downloads/,"Based on LLaMA2 weights, which are made conditionally available by Meta.",open,https://huggingface.co/datasets/WizardLM/WizardLM_evol_instruct_V2_196k,The Evol-Instruct V2 dataset contains 196k instruction-following sequences generated from Evol-Instruct,open,https://huggingface.co/WizardLM/WizardLM-13B-V1.2,Model weights offered in HuggingFace repository,partial,https://github.com/nlpxucan/WizardLM/blob/main/WizardLM/MODEL_DIFF_LICENSE,"Restricted for academic research purposes only. Code and Model diff release under CC-BY-NC-4.0, software code under Apache 2.0",partial,https://github.com/nlpxucan/WizardLM/tree/main/WizardLM,"Code is only partially documented, not clearly versioned, and appears to be in flux.",open,https://arxiv.org/abs/2304.12244,Architecture described in preprint and partly accessible in code repository,open,https://arxiv.org/abs/2304.12244,Preprint describes method for creating large amounts of LLM-based synthetic RLHF data and fine-tuning WizardLM based on it,closed,,No peer-reviewed paper or data audit found,closed,https://huggingface.co/WizardLM/WizardLM-13B-V1.2,Model card is only a placeholder and generates an error (missing yaml metadata),closed,https://huggingface.co/datasets/WizardLM/WizardLM_evol_instruct_V2_196k,Dataset card for Evol-Instruct generates an error,closed,,No package available,closed,,No API available,/projects/wizardlm-13B.yaml,6.0
https://huggingface.co/jondurbin/airoboros-l2-70b-gpt4-1.4.1,,Llama2,Airoboros (synthetic),Purposely left ambiguous,Jon Durbin,https://github.com/jondurbin,Only active on GitHub since May 2023,partial,https://gist.github.com/jondurbin/87fc040b92a3073125ed516b04bc6e19,Repo exists for RL data but only a gist exists for model training and architecture,closed,,Llama2 training data is nowhere documented or disclosed,partial,,"Llama2, made conditionally available by Meta",open,https://github.com/jondurbin/airoboros,"Airoboros, an implementation of the Self-Instruct paper",open,https://huggingface.co/jondurbin/airoboros-l2-70b-gpt4-1.4.1/tree/main,Made available through HuggingFace,partial,https://huggingface.co/jondurbin/airoboros-l2-70b-gpt4-1.4.1#licence-and-usage-restrictions,Licensing left ambiguous because of murky status of OpenAI-derived Self-Instruct data,partial,,What little code available is not very systematically documented,partial,https://huggingface.co/jondurbin/airoboros-l2-70b-gpt4-1.4.1/discussions/2#64c29e4c617b36543dedac9a,Some info can be gleaned at link but most remains undocumented,closed,,No preprint found,closed,,No peer-reviewed paper found,partial,https://huggingface.co/jondurbin/airoboros-65b-gpt4-1.4,Instructs reader to look up model card for prior 65B Llama1 version,partial,https://huggingface.co/datasets/jondurbin/airoboros-gpt4-1.4.1,Datasheet for RL data only,closed,,No package found,closed,,No API found,/projects/airoboros.yaml,5.5
https://github.com/THUDM/ChatGLM-6B/blob/main/README_en.md,"From the readme, ""ChatGLM-6B uses technology similar to ChatGPT, optimized for Chinese QA and dialogue. The model is trained for about 1T tokens of Chinese and English corpus, supplemented by supervised fine-tuning, feedback bootstrap, and reinforcement learning wit human feedback. With only about 6.2 billion parameters, the model is able to generate answers that are in line with human preference.""",GLM (own),Unspecified,Apache 2.0,THUDM,https://github.com/THUDM,Knowledge Engineering Group (KEG) & Data Mining at Tsinghua University,partial,https://github.com/THUDM/ChatGLM-6B/blob/main/README_en.md#deployment,Some code made available on Github,partial,http://doi.org/10.18653/v1/2022.acl-long.26,"Training data not centrally made available, but described in 2022 ACL paper, appears to be mostly public datasets",open,https://huggingface.co/THUDM/chatglm-6b/tree/main,Model made available through HuggingFace,closed,,"docs mention ""supervised fine-tuning, feedback bootstrap, and reinforcement learning wit human feedback"", but none of the datasets used are clearly specified.",closed,,No weights or checkpoints corresponding to the delta of the LLM vs RLHF provided,open,https://github.com/THUDM/ChatGLM-6B/blob/main/LICENSE,Apache 2.0,partial,https://github.com/THUDM/ChatGLM-6B/blob/main/ptuning/README_en.md,"Some documentation available, but a lot of code is not commented or explained.",partial,,Full details of architecture not specified in a single place,closed,,,partial,https://aclanthology.org/2022.acl-long.26/,"ACL 2022 paper describes the training of the GLM base model, but the RLHF portion is more recent (there is also a related ICLR paper for a newer generation https://openreview.net/forum?id=-Aw0rrrPUF)",closed,https://huggingface.co/THUDM/chatglm-6b,No modelcard; the HuggingFace modelcard spot is used just as the homepage for the model.,closed,,No datasheet,closed,,No package,open,https://github.com/THUDM/ChatGLM-6B/blob/main/README_en.md#api-deployment,API provided through fastapi uvicorn,/projects/ChatGLM-6B.yaml,5.5
https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1,,unclear,unspecified,Apache 2.0,Mistral AI,https://mistral.ai/,,partial,https://github.com/mistralai/mistral-src,repository provides 'minimal code to run our 7B model',closed,,No information provided on pretraining data,open,https://github.com/mistralai/mistral-src#download-the-model,Base LLM model made available for download,closed,https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1,No information provided expect that instruction tuning is done using an unspecified 'variety of publicly available conversation datasets',partial,https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1/tree/main,Instruct version of the model made available but no information on fine-tuning procedure provided,open,https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1/blob/main/README.md,Apache 2.0,closed,https://github.com/mistralai/mistral-src,the little code that is available is uncommented and undocumented,partial,https://github.com/mistralai/mistral-src,Some information on architecture provided in github repo,partial,http://arxiv.org/abs/2310.06825,"Preprint rehashes marketing blurbs also given in blog and provides no details about pretraining datasets, instruction tuning datasets, or fine-tuning process, hence partial.",closed,,No peer reviewed paper available,closed,,"No model card available, HuggingFace modelcard just points to a corporate blog post",closed,,No datasheet available,partial,https://docs.mistral.ai/quickstart/,Docker image shared on github,open,https://docs.mistral.ai/api,API specification provided by vLLM,/projects/mistral-7B.yaml,5.5
https://github.com/nlpxucan/WizardLM,Empowering Large Pre-Trained Language Models to Follow Complex Instructions,LLaMA-7B,Evol-Instruct (synthetic),CC-BY-NC-4.0,Microsoft & Peking University,https://github.com/nlpxucan,,partial,https://github.com/nlpxucan/WizardLM/tree/main/WizardLM,Fast-evolving repository contains WizardLM code,partial,,"Based on LLaMA, which is claimed to be public but nowhere exactly documented.",closed,,"Based on LLaMA weights, which are not openly available though a leaked versions is in wide circulation.",open,https://github.com/nlpxucan/WizardLM/tree/main/WizardLM#training-data,The Evol-Instruct dataset contains 70k instruction-following sequences generated from Evol-Instruct,partial,https://huggingface.co/WizardLM/WizardLM-7B-V1.0/tree/main,Model weights offered as a delta to LLaMA,partial,https://github.com/nlpxucan/WizardLM/blob/main/WizardLM/MODEL_DIFF_LICENSE,"Restricted for academic research purposes only. Code and Model diff release under CC-BY-NC-4.0, software code under Apache 2.0",partial,https://github.com/nlpxucan/WizardLM/tree/main/WizardLM,"Code is only partially documented, not clearly versioned, and appears to be in flux.",open,https://arxiv.org/abs/2304.12244,Architecture described in preprint and partly accessible in code repository,open,https://arxiv.org/abs/2304.12244,Preprint describes method for creating large amounts of LLM-based synthetic RLHF data and fine-tuning WizardLM based on it,closed,,No peer-reviewed paper or data audit found,closed,https://huggingface.co/WizardLM/WizardLM-7B-V1.0,Model card is only a placeholder and generates an error (missing yaml metadata),closed,https://huggingface.co/datasets/WizardLM/WizardLM_evol_instruct_V2_196k,Dataset card for Evol-Instruct generates an error,closed,,No package available,closed,,No API available,/projects/wizardlm-7B-V1.yaml,5.5
https://mistral.ai/news/mistral-nemo/,,Mistral NeMo,unspecified,Apache 2.0 (model weights only),Mistral AI,https://mistral.ai/,,partial,https://github.com/mistralai/mistral-inference,repository provides 'minimal code to run our models',closed,,No information provided on pretraining data,open,https://models.mistralcdn.com/mistral-nemo-2407/mistral-nemo-base-2407.tar,Base LLM model made available for download,closed,https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1,No information provided expect that instruction tuning is done using an unspecified 'variety of publicly available conversation datasets',partial,https://models.mistralcdn.com/mistral-nemo-2407/mistral-nemo-instruct-2407.tar,Instruct version of the model made available but no information on fine-tuning procedure provided,open,https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1/blob/main/README.md,Apache 2.0,closed,https://github.com/mistralai/mistral-inference,the little code that is available is uncommented and undocumented,partial,https://github.com/mistralai/mistral-inference,Some information on architecture provided in github repo and in release blogpost,closed,,No preprint found,closed,,No peer reviewed paper available,closed,,No model card available,closed,,No datasheet available,partial,https://docs.mistral.ai/quickstart/,Docker image shared on github,open,https://docs.mistral.ai/api,API specification provided by vLLM,/projects/mistral-nemo.yaml,5.0
https://qwenlm.github.io/blog/qwen1.5/,"This is based on the 72B version, the largest of 8 available model sizes.",QwenLM,Unspecified,Qianwen License,Alibaba Cloud,,Qwen (abbr. for Tongyi Qianwen 通义千问) refers to the large language model family built by Alibaba Cloud,partial,https://github.com/QwenLM/Qwen1.5/,Repository provides sparse source code and some examples for SFT,closed,,Pretraining data not specified or documented.,open,https://huggingface.co/Qwen/Qwen1.5-72B/tree/main,Also available in smaller model sizes,closed,https://qwen.readthedocs.io/en/latest/training/SFT/llama_factory.html,Data not specified or documented. Some example code in repo provides directions but no details.,open,https://huggingface.co/Qwen/Qwen1.5-72B-Chat/tree/main,Also available in smaller model sizes,closed,,Qianwen License,partial,,Repository is fairly well-documented.,partial,,No clear description of architecture found.,closed,,No preprint found.,closed,,No peer-reviewed paper found.,closed,,"Model card on HF only serves as a pointer to the model, no actual info provided.",closed,,No datasheet.,partial,,No specific package provided but integrates well with many widely used packages,open,,Available through various APIs,/projects/qwen-1.5-chat.yaml,5.0
https://huggingface.co/CarperAI/stable-vicuna-13b-delta,StableVicuna-13B is a Vicuna-13B v0 model fine-tuned using reinforcement learning from human feedback (RLHF) via Proximal Policy Optimization (PPO) on various conversational and instructional datasets,LLaMA,"OASST1 (human), GPT4All (human), Alpaca (synthetic)",,CarperAI,https://carper.ai,,partial,https://huggingface.co/CarperAI/stable-vicuna-13b-delta/tree/main,Some elements of the code made available through HuggingFace,closed,https://huggingface.co/CarperAI/stable-vicuna-13b-delta,Based on LLaMA whose pretraining data has nowhere been disclosed or documented.,partial,https://huggingface.co/CarperAI/stable-vicuna-13b-delta#apply-delta-weights,"Model not functional out of the box as weights require a delta computation. From the docs 'StableVicuna-13B cannot be used from the CarperAI/stable-vicuna-13b-delta weights alone. To obtain the correct model, one must add back the difference between LLaMA 13B and CarperAI/stable-vicuna-13b-delta weights.'",partial,https://huggingface.co/CarperAI/stable-vicuna-13b-delta,"From the documentation 'The reward model used during RLHF was also trained on OpenAssistant Conversations Dataset (OASST1) along with two other datasets Anthropic HH-RLHF, a dataset of preferences about AI assistant helpfulness and harmlessness; and Stanford Human Preferences Dataset a dataset of 385K collective human preferences over responses to questions/instructions in 18 different subject areas, from cooking to legal advice.'",partial,https://huggingface.co/CarperAI/stable-vicuna-13b-delta/discussions/7,The HuggingFace community page has an open question for release of the RL model,partial,https://huggingface.co/CarperAI/stable-vicuna-13b-delta,"CC-BY-NC-SA-4.0. License for LLaMA is more murky, hence partial. As they say 'License for the base LLaMA model's weights is Meta's non-commercial bespoke license.'",partial,https://huggingface.co/CarperAI/stable-vicuna-13b-delta/tree/main,"Code is minimally documented and deployment requires non-trivial configuration, e.g. 'StableVicuna-13B cannot be used from the CarperAI/stable-vicuna-13b-delta weights alone. To obtain the correct model, one must add back the difference between LLaMA 13B and CarperAI/stable-vicuna-13b-delta weights.'",partial,,"Architecture is described in scattered places, but there is no clear and exhaustive overview.",partial,https://arxiv.org/abs/2302.13971,"Preprint covers only the LLaMA base model, hence partial.",closed,,No paper found.,partial,https://huggingface.co/lmsys/vicuna-13b-delta-v0,Model card provides some information but is not fully worked out as recommended in model card literature.,closed,,No datasheet found,closed,,No package found,partial,https://github.com/lm-sys/FastChat/tree/main#api,Addressable via FastChat / HuggingFace API,/projects/stablevicuna.yaml,5.0
https://huggingface.co/tiiuae/falcon-40b-instruct,,Falcon 40B,Baize (synthetic),Apache 2.0 license,Technology Innovation Institute,https://falconllm.tii.ae,,closed,https://huggingface.co/tiiuae/falcon-40b-instruct,"No source code shared, even though the term ""open source"" is used.",partial,https://huggingface.co/datasets/tiiuae/falcon-refinedweb,"From the documentation 'The key ingredient for the high quality of the Falcon models is their training data, predominantly based (>80%) on RefinedWeb — a novel massive web dataset based on CommonCrawl' (https://huggingface.co/blog/falcon). However, only a small sample is made available.",open,https://huggingface.co/tiiuae/falcon-40b-instruct/tree/main,Model weights available through HuggingFace library,partial,https://github.com/project-baize/baize-chatbot,RL data inherited from Baize but provenance not well-documented. From the documentation 'Falcon-40B-Instruct was finetuned on a 150M tokens from Baize mixed with 5% of RefinedWeb data.',closed,https://github.com/project-baize/baize-chatbot#v1,No RL weights or checkpoints made available,open,,First release came with a legally murky license that was swiftly criticised and now generates a 404. Current documentation 'Falcon-40B-Instruct is made available under the Apache 2.0 license.',closed,,"No source code found, therefore no documentation found.",partial,,"Architecture sketched on HuggingFace as ""Falcon-40B-Instruct is a 40B parameters causal decoder-only model built by TII based on Falcon-40B and finetuned on a mixture of Baize.""",partial,https://arxiv.org/abs/2306.01116,"Preprint covers the creation and curation of RefinedWeb dataset, but not other aspects of the model, hence partial.",closed,,No peer-reviewed paper known.,partial,https://huggingface.co/tiiuae/falcon-40b-instruct,"Model card on HuggingFace is mostly used to advertise the model, not to document its training and evaluation details.",closed,,There is no datasheet available.,closed,,There is no package.,closed,https://huggingface.co/tiiuae/falcon-40b-instruct,"There is no API, and HuggingFace inference API is disabled for this model.",/projects/Falcon-40B-instruct.yaml,4.5
https://huggingface.co/openbmb/UltraRM-13b,,LLaMA2,UltraFeedback (part synthetic),Meta Community License,OpenBMB,https://www.openbmb.org/,"OpenBMB (Open Lab for Big Model Base), founded by TsinghuaNLP & ModelBest Inc (面壁智能), aims to build foundation models and systems towards AGI.",closed,,,closed,,"Based on Llama2, which means pretraining data is nowhere disclosed or documented, and described only in the vaguest terms in a corporate preprint released by Meta",partial,https://ai.meta.com/resources/models-and-libraries/llama-downloads/,Download only after requesting access; requires signing a consent form,open,https://huggingface.co/datasets/openbmb/UltraFeedback,UltraFeedback dataset made available along with model release,partial,https://huggingface.co/openbmb/UltraLM-13b-v2.0,"Online materials appear to be in flux and several HuggingFace links generate 404 errors, hence partial",closed,https://huggingface.co/openbmb/UltraLM-13b#model-details,"Usage requires signing Meta's bespoke 'community license', not an OSI recognised open license",closed,,Code only covers minimal examples; no documentation available.,partial,https://huggingface.co/openbmb/UltraLM-13b#model-details,Architecture sketched in online materials.,open,https://arxiv.org/abs/2310.01377,Preprint describes creation of UltraFeedback dataset but also offers some detail on training and architecture of UltraRM models,closed,,No peer-reviewed paper found,partial,https://huggingface.co/openbmb/UltraLM-13b#model-details,"Model card on HuggingFace available for 13b (Llama1) model, but not for newer releases, hence partial",partial,https://huggingface.co/datasets/openbmb/UltraFeedback,"Datasheet available for RLHF portion, but not for Llama2-based pretraining data, hence partial",closed,,Package not provided,closed,,API not provided and model too big for HuggingFace inference API,/projects/ultraLM.yaml,4.5
https://huggingface.co/01-ai/Yi-34B-Chat,The Yi series models are the next generation of open-source large language models trained from scratch by 01.AI. Targeted as a bilingual language model and trained on 3T multilingual corpus.,Yi 34B,unspecified,Apache 2.0 and bespoke Yi Series Community License Agreement 2.1,01.AI,https://www.01.ai/,,partial,https://github.com/01-ai/Yi,"repository contains some code for demos and for instruction tuning, but only sparse examples of code for initial training and model architecture",closed,,"Training data for base model undocumented, though preprint mentions CommonCrawl as one source",open,https://huggingface.co/01-ai/Yi-34B/tree/main,Base model weights shared via HuggingFace,closed,https://huggingface.co/datasets/BAAI/COIG,"Model docs mention COIG but only as an example. Code also shows rm-static is also used in instruction tuning, but not shared.",open,https://huggingface.co/01-ai/Yi-34B-Chat/tree/main,Instruction-tuned weights shared via HuggingFace,partial,,"Model code licensed under Apache 2.0, model weights licensed under Llama-inspired community license with limitations",closed,https://github.com/01-ai/Yi/,"Documentation only describes how to deploy, training and fine-tuning code not available or where available not commented or documented",closed,,Model architecture only described in the most general terms.,open,https://arxiv.org/abs/2403.04652,Preprint provides some info on pretraining data (CommonCrawl) but none on instruction tuning dataset.,closed,,No paper found,closed,,"Model card used for advertising and navigation purposes, does not follow general model card specifications.",closed,,No data sheets found.,closed,,no package found,partial,,"limited access via HF API, otherwise gated and commercially available",/projects/yi-chat.yaml,4.5
https://bair.berkeley.edu/blog/2023/04/03/koala/,"From the documentation 'Koala is fine-tuned on freely available interaction data scraped from the web, but with a specific focus on data that includes interaction with highly capable closed-source models such as ChatGPT.'",LLaMA 13B,"HC3, ShareGPT, alpaca (synthetic)",Mixed,BAIR,https://bair.berkeley.edu/,Berkeley Artificial Intelligence Research,open,https://github.com/young-geng/EasyLM,Code scattered across projects and repositories,partial,https://github.com/young-geng/koala_data_pipeline,Repo contains data pipeline for preprocessing. Based on LLaMA which is said to be based on 'publicly available datasets' which are not made directly available.,partial,https://drive.google.com/drive/folders/10f7wrlAFoPIy-TECHsx9DKIvbQYunCfl,Model weights only made available as a diff against LLaMA. OpenLLaMA provides a possible alternative?,partial,https://bair.berkeley.edu/blog/2023/04/03/koala/#datasets-and-training,Datasets described in blog post but not all made available,closed,,RL weights not made available separately,partial,https://huggingface.co/young-geng/koala#license,"Licensing is 'subject to the model License of LLaMA, Terms of Use of the data generated by OpenAI, and Privacy Practices of ShareGPT'",partial,https://github.com/young-geng/EasyLM,Code scattered across various repositories and not systematically documented.,partial,https://github.com/young-geng/EasyLM,Architecture visually illustrated and described in some detail.,closed,,No preprint available,closed,,No peer-reviewed paper,closed,,No model card available,closed,,No systematic data sheet available,closed,,No package available,closed,,No API available,/projects/koala.yaml,4.0
https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1,,Mistral,Unspecified,Apache 2.0,Mistral AI,https://mistral.ai,,closed,,No training code for Mixtral 8x7B made available,closed,,No information provided on pretraining data,open,https://huggingface.co/mistralai/Mixtral-8x7B-v0.1/tree/main,Weights shared via torrent and via HuggingFace,closed,https://huggingface.co/mistralai/Mixtral-8x7B-v0.1/tree/main,No information provided online or in the preprint,partial,https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1/tree/main,Instruct version of model weights made available but no information on fine-tuning procedure provided,open,https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1/blob/main/README.md,Apache 2.0,closed,https://github.com/mistralai/mistral-src,"the little code that is available is uncommented and undocumented, and relates to a reference implementation of the 7B version",partial,https://github.com/mistralai/mistral-src,Some information on architecture provided in github repo and in preprint,partial,https://arxiv.org/abs/2401.04088,"Preprint describes mixture of experts and evalutaion but provides no details about pretraining datasets, instruction tuning datasets, or fine-tuning process, hence partial.",closed,,No peer reviewed paper available,closed,,"No model card available, HuggingFace modelcard just points to a corporate blog post",closed,,No datasheet available,partial,https://docs.mistral.ai/quickstart/,Docker image shared on github,closed,https://console.mistral.ai/,Only paid API access available,/projects/mixtral-8x7B-instruct.yaml,4.0
https://huggingface.co/stabilityai/StableBeluga2,,LLaMA2,Orca-style (synthetic),Stable Beluga Non-Commercial Community License,Stability AI,https://stability.ai/,,closed,,"No repository with open code related to training, fine-tuning or evaluation found",closed,,"Pretraining data nowhere disclosed or documented, and described only in the vaguest terms in a corporate preprint released by Meta",partial,https://ai.meta.com/resources/models-and-libraries/llama-downloads/,Download only after requesting access; requires signing a consent form,closed,,"Says: 'Stable Beluga 2 is trained on our internal Orca-style dataset', 'created synthetically using high-quality instructions' from COT Submix Original, NIV2 Submix Original, FLAN 2021 Submix Original, T0 Submix Original.",open,https://huggingface.co/stabilityai/StableBeluga2/tree/main,Instruction-tuned model weights available,partial,https://huggingface.co/stabilityai/StableBeluga2/blob/main/LICENSE.txt,"Usage requires signing StabilityAI's bespoke 'Stable Beluga Non-Commercial Community License', not an OSI recognised open license",closed,,Code on HuggingFace only covers minimal examples; no documentation available.,partial,https://huggingface.co/stabilityai/StableBeluga2#model-details,Says 'Stable Beluga 2 is a Llama2 70B model finetuned on an Orca style Dataset'.,partial,https://huggingface.co/papers/2306.02707,"Preprint from Microsoft describes Orca method of finetuning using GPT4-derived synthetic data, but no details of this particular architecture",closed,,No peer-reviewed paper found,partial,https://huggingface.co/stabilityai/StableBeluga2,"There is a model card, but it provides only a minimum of detail",closed,,Datasheet not provided.,closed,,Package not provided,partial,,model too large to run on HuggingFace free inference API,/projects/StableBeluga2.yaml,4.0
https://crfm.stanford.edu/2023/03/13/alpaca.html,project_notes,LLaMA,Self-Instruct (synthetic),LLaMA license agreement,Stanford University CRFM,https://crfm.stanford.edu/,,open,,,closed,,"Based on LLaMA, whose pretraining data is nowhere disclosed or documented.",partial,,"LLaMA based, copyright status unclear",partial,https://github.com/tatsu-lab/stanford_alpaca#data-release,alpaca_data.json contains 52K instruction-following data we used for fine-tuning the Alpaca model.,partial,https://github.com/tatsu-lab/stanford_alpaca#data-release,LLaMA based,closed,https://docs.google.com/forms/d/e/1FAIpQLSfqNECQnMkycAp2jP4Z9TFX0cGR4uf7b_fBxjY_OjhJILlKGA/viewform,Pegged to LLaMA licence agreement,partial,https://github.com/tatsu-lab/stanford_alpaca,Insofar as code is made available it is fairly well documented,open,https://github.com/tatsu-lab/stanford_alpaca#fine-tuning,Fair bit of documentation available on github repository,closed,,No preprint found; uses the release-by-blogpost playbook,closed,,No peer-reviewed work found.,closed,,No model card found,closed,,No data sheet found,closed,,,closed,,,/projects/alpaca.yaml,4.0
https://huggingface.co/tiiuae/falcon-180B-chat,"Falcon-180B-Chat is a 180B parameters causal decoder-only model built by TII based on Falcon-180B and finetuned on a mixture of Ultrachat, Platypus and Airoboros.",Falcon 180B,"OpenPlatypus, Ultrachat, Airoboros (synthetic)",FALCON 180B TII LICENSE VERSION 1.0,Technology Innovation Institute,https://falconllm.tii.ae,,closed,https://huggingface.co/tiiuae/falcon-180B-chat,No source code shared anywhere. The over 200 github repositories of TII appear to include no LLM or Falcon related code.,partial,https://huggingface.co/datasets/tiiuae/falcon-refinedweb,"From the documentation 'The key ingredient for the high quality of the Falcon models is their training data, predominantly based (>80%) on RefinedWeb — a novel massive web dataset based on CommonCrawl' (https://huggingface.co/blog/falcon). However, only a small sample is made available.",partial,https://huggingface.co/tiiuae/falcon-180B,"requires signing up and accepting ""acceptable use policy""",partial,https://github.com/project-baize/baize-chatbot,"No details provided beyond ""finetuned on a mixture of Ultrachat, Platypus and Airoboros"".",partial,https://huggingface.co/tiiuae/falcon-180B-chat/tree/main,"No RL weights or checkpoints made available; fine-tuned 'chat' model only available after signing ""acceptable use policy"".",closed,https://huggingface.co/spaces/tiiuae/falcon-180b-license/blob/main/LICENSE.txt,"Released under Falcon 180B TII license (not OSI approved) and a separate ""acceptable use policy"".",closed,,"No code available, so no code documented.",partial,https://huggingface.co/tiiuae/falcon-180B-chat#model-architecture-and-objective,"Architecture described as  ""causal decoder-only"" and ""optimized for inference"", but very few details available.",partial,https://arxiv.org/abs/2306.01116,"Preprint covers the creation and curation of RefinedWeb dataset, but not other aspects of the model, hence partial.",closed,,No peer-reviewed paper known.,partial,https://huggingface.co/tiiuae/falcon-180B-chat,"Model card on HuggingFace is mostly used to advertise the model, not to document its training and evaluation details.",closed,,There is no datasheet available.,closed,,There is no package.,closed,https://huggingface.co/tiiuae/falcon-180B-chat,"There is no API, and HuggingFace inference API is disabled for this model.",/projects/Falcon-180B-chat.yaml,3.5
https://ai.google.dev/gemma/docs,Model weights and developer tools,Gemma,Unspecified,,Google DeepMind,https://ai.google.dev/gemma/docs,,partial,https://github.com/google-deepmind/gemma,No pre-training or instructing-tuning code made available. Some developer tools available.,closed,,No details provided on pre-training data.,partial,https://www.kaggle.com/models/keras/gemma/frameworks/keras/variations/gemma_7b_en,"Base model weights shared via Kaggle, requires privacy-defying access request.",closed,,"Documentation says 'These versions of the model are trained with human language interactions and can respond to conversational input, similar to a chat bot.' ",partial,,"Instruction-tuned model weights shared via Kaggle, requires privacy-defying access request",closed,https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/335?pli=1,"Bespoke Gemma Community License Agreement and restrictive Terms of Use, neither open in the sense of OSI. Only Inference code shared under Apache 2.0.",closed,,No pretraining or finetuning code found. No code documentation except for deployment of the open weights.,partial,https://www.kaggle.com/models/google/gemma,Architecture described in very general terms in model card,partial,https://arxiv.org/abs/2403.08295,"Preprint released Apr 2024 details architecture and evaluation, but provides no information on pre-training, instruction tuning and SFT datasets",closed,,No paper found,open,https://www.kaggle.com/models/google/gemma,Model card on Kaggle provides some detail on model internals and evaluation,closed,,"No datasheet found, pre-training and instruction-tuning data nowhere specified.",closed,,"No package provided, access is gated through Kaggle, Vertex Model Garden, Google Cloud",closed,,"No API provided, access gated through Kaggle, Vertex Model Garden, Google Cloud",/projects/gemma-instruct.yaml,3.5
https://www.microsoft.com/en-us/research/project/orca/,"This file applies to Orca 2 7B and 13B, both fine-tunes of corresponding Llama2 base models.",LLaMA2,"FLAN, Math, undisclosed (synthetic)","Microsoft Research License, Llama Community License",Microsoft Research,https://www.microsoft.com/en-us/research/project/orca/,,closed,,"No source code made available anywhere for data curation, training, fine-tuning, or evaluation",closed,,"Llama pretraining Data nowhere disclosed or documented, and described only in the vaguest terms in a corporate preprint released by Meta",partial,https://ai.meta.com/resources/models-and-libraries/llama-downloads/,Download only after requesting access; requires signing a consent form,closed,,"Instruction-tuning and explanation-tuning data described in preprint, but not disclosed or made available",open,https://huggingface.co/microsoft/Orca-2-7b,7B and 13B finetunes of Llama2 made available through HuggingFace,closed,https://github.com/facebookresearch/llama/blob/main/LICENSE,"Licensed under Microsoft Research License and Meta's bespoke community license, neither OSI recognised",closed,,Code only covers minimal examples; no documentation available.,partial,https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/,"Architecture sketched in corporate preprints, though many details missing.",partial,https://arxiv.org/pdf/2311.11045.pdf,"Corporate preprint from Microsoft Research has some detail on instruction-tuning, explanation-tuning and evaluation",closed,,No peer-reviewed paper found,partial,https://huggingface.co/microsoft/Orca-2-7b,Model card serves as landing page more than careful model documentation,closed,,Datasheet not provided.,closed,,Package not provided,partial,,Model too large to load on HuggingFace inference library,/projects/orca_2.yaml,3.5
https://huggingface.co/CohereForAI/c4ai-command-r-v01,,,Aya Collection,CC-BY-NC and C4AI acceptable use policy,Cohere AI,https://cohere.com,,closed,,"No codebase available to study or adjust model architecture, training, or inner workings.",closed,https://docs.cohere.com/docs/data-statement,"No documentation, listing or audit of pre-training data available. Cohere itself identifies it as coheretext-filtered and gives the size as 200Gb.",closed,,No checkpoint or model prior to SFT and instruction-tuning made available,open,https://huggingface.co/collections/CohereForAI/aya-datasets-660415741bd4852f01c81c77,Aya Collection (Aya Open Science initiative) is a multilingual collection of 513 million instances of promts and completions including instruction-style templates.,open,https://huggingface.co/CohereForAI/c4ai-command-r-v01/tree/main,Fine-tuned model weights made available for download,partial,https://docs.cohere.com/docs/c4ai-acceptable-use-policy,Licensed under CC-BY-NC and requires agreeing to C4AI acceptable use policy,closed,,"No source code available, so no documentation of code.",closed,,Architecture only sparsely documented.,closed,,No preprint appears to be made available at this time.,closed,,No paper known to document the Cohere Command R+ model or architecture.,partial,https://huggingface.co/CohereForAI/c4ai-command-r-v01-4bit,"Model card on HF document some aspects but provides no data on training data, instruction-tuning methods",closed,,Datasheet not available.,closed,,No separate package available.,closed,,API access available only when signing up.,/projects/command-r.yaml,3.0
https://ai.meta.com/resources/models-and-libraries/llama/,,LLaMA2,"Meta, StackExchange, Anthropic",Unclear,Facebook Research,https://github.com/facebookresearch,,closed,https://github.com/facebookresearch/llama/tree/main,"Repository only offers 'a minimal example to load Llama 2 models and run inference'; no training, fine-tuning, evaluation code made available",closed,,"Data nowhere disclosed or documented, and described only in the vaguest terms in a corporate preprint released by Meta",partial,https://ai.meta.com/resources/models-and-libraries/llama-downloads/,Download only after requesting access; requires signing a consent form,closed,,RLHF data including 1 million Meta-specific tuning prompts not made available (even as it incorporates some open RLHF datasets),partial,https://ai.meta.com/resources/models-and-libraries/llama-downloads/,Download only after requesting access; requires signing a consent form,closed,https://github.com/facebookresearch/llama/blob/main/LICENSE,"Usage requires signing Meta's bespoke 'community license', not an OSI recognised open license",closed,,Code only covers minimal examples; no documentation available.,partial,https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/,"Architecture sketched in preprint, though many details missing.",partial,https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/,"Corporate preprint quite some detail on pretraining, RLHF, and safety measures but none on datasets.",closed,,No peer-reviewed paper found,partial,https://github.com/facebookresearch/llama/blob/main/MODEL_CARD.md,"There is a model card, but it provides the absolute minimum of detail, and none whatsoever on training data.",closed,,Datasheet not provided.,closed,,Package not provided,partial,,API only available behind a privacy-defying signup form,/projects/llama-2-chat.yaml,3.0
https://huggingface.co/Nanbeige/Nanbeige2-8B-Chat,Comes in 8B and 16B versions,Unknown,Unknown,Apache 2.0 and bespoke community license,Nanbeige LLM lab,https://huggingface.co/Nanbeige,,open,https://github.com/Nanbeige/Nanbeige,"github repo contains sparse but clear code for training, tuning, and inference",closed,,No information on pre-training datasets except a claim of 4.5T tokens. Request for information on HF community was closed without comment.,closed,,Base model not shared,closed,https://huggingface.co/Nanbeige/Nanbeige2-8B-Chat/discussions/2#6621e15a4d17641cf788cbd5,"No information on finetuning and DPO datasets. Some information provided on request (see link), but official documentation not updated.",open,https://huggingface.co/Nanbeige/Nanbeige2-8B-Chat/tree/main,Model weights for finetuned model shared,partial,,Apache 2.0 but commercial use requires signup and an additional community license,closed,,No documentation of the codebase,closed,,Architecture not clearly specified,closed,,No preprint found,closed,,No paper found,closed,,No model card found,closed,,No datasheet found,closed,,No package found,partial,https://huggingface.co/spaces/Nanbeige/Nanbeige-Plus-Chat-v0.1,"No API, but HuggingFace space available",/projects/nanbeige-chat.yaml,3.0
https://huggingface.co/collections/meta-llama/meta-llama-3-66214712577ca38149ebb2b6,,Meta Llama 3,"Meta, undocumented",Meta Llama 3 Community License,Facebook Research,https://github.com/facebookresearch,,closed,https://github.com/meta-llama/llama3,Repository only offers minimal code,closed,,"Data nowhere disclosed or documented, and described only in the vaguest terms in a release blog post",partial,https://ai.meta.com/resources/models-and-libraries/llama-downloads/,Download only after requesting access; requires signing a consent form,closed,,No information available on instruction-tuning.,partial,https://ai.meta.com/resources/models-and-libraries/llama-downloads/,Download only after requesting access; requires signing a consent form,closed,https://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct/tree/main,"Even inspecting the model requires signing Meta Llama 3's bespoke 'community license', not an OSI recognised open license",closed,,Code only covers minimal examples; no documentation available.,partial,https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/,Architecture sketched in glossy blog post.,closed,,No preprint or any other scientific documentation available.,closed,,No peer-reviewed paper available.,partial,https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md,"There is a model card, but it provides the absolute minimum of detail, and none whatsoever on training data.",closed,,Datasheet not provided.,closed,,Package not provided,partial,,API only available behind a privacy-defying signup form,/projects/llama-3-instruct.yaml,2.5
https://huggingface.co/upstage/SOLAR-0-70b-16bit,HuggingFace profile says 'Solar is a great example of the progress enabled by open source.',LLaMA2,"Orca-style, Alpaca-style","Meta Community license, CC-BY-NC",Upstage AI,https://en.upstage.ai/,Korean venture,closed,,No code repository found,closed,,"Data nowhere disclosed or documented, and described only in the vaguest terms in a corporate preprint released by Meta",partial,https://ai.meta.com/resources/models-and-libraries/llama-downloads/,Download only after requesting access; requires signing a consent form,closed,,"No RLHF datasets specified or shared, docs say 'Orca-style dataset, Alpaca-style dataset'",partial,https://huggingface.co/upstage/SOLAR-0-70b-16bit/tree/main,Finetuned checkpoints only shared through CC-BY-NC,closed,https://huggingface.co/upstage/SOLAR-0-70b-16bit#model-details,"Meta Community License for base model, and CC-BY-NC 4.0 for fine-tuned model weights",closed,,HuggingFace code only comprises configuration json; no documentation available.,closed,https://huggingface.co/upstage/SOLAR-0-70b-16bit,"Precise architecture, training, fine-tuning procedures not given.",closed,,No preprint or any form of scientific docuentation found.,closed,,No peer-reviewed paper found,partial,https://huggingface.co/upstage/SOLAR-0-70b-16bit,"HuggingFace model card used mostly as advertising, omits many details on training, fine-tuning, evaluation.",closed,,Datasheet not provided.,closed,,Package not provided,partial,,API only available by signing up for 'private LLM' service,/projects/solar-70B.yaml,2.0
https://huggingface.co/Xwin-LM/Xwin-LM-7B-V0.1,Xwin-LM aims to develop and open-source alignment technologies for large language models,LLaMA2,unknown,Llama 2 license,Xwin-LM,https://github.com/Xwin-LM,Xwin-LM aims to develop and open-source alignment technologies for large language models,closed,https://huggingface.co/Xwin-LM/Xwin-LM-7B-V0.1,"HuggingFace page notes 'to do "":"" Release the source code'",closed,,"Data nowhere disclosed or documented, and described only in the vaguest terms in a corporate preprint released by Meta",partial,https://ai.meta.com/resources/models-and-libraries/llama-downloads/,Download only after requesting access; requires signing a consent form,closed,,"RLHF data for Llama includes 1 million Meta-specific tuning prompts not made available, no other details known about RLHF and alignment tuning added by Xwin-LM",closed,https://huggingface.co/Xwin-LM/Xwin-LM-70B-V0.1/tree/main,Downloadable model presumably includes RLHF tuning but no documentation available,closed,https://github.com/facebookresearch/llama/blob/main/LICENSE,"Usage requires signing Meta's bespoke 'community license', not an OSI recognised open license",closed,,No documentation available.,closed,https://github.com/Xwin-LM/Xwin-LM#news,"No information available beyond that it is based on Llama and ""RLHF plays crucial role""",closed,https://github.com/Xwin-LM/Xwin-LM#news,"No preprint available; ""Coming soon (Stay tuned)""",closed,,No peer-reviewed paper available,closed,https://huggingface.co/Xwin-LM/Xwin-LM-70B-V0.1,"HuggingFace ""model card"" used to advertise model, but no details available.",closed,,Datasheet not provided.,closed,,Package not provided,partial,,API available through vllm and HuggingFace,/projects/Xwin-LM.yaml,1.0
https://chat.openai.com/,NA,GPT 3.5,Instruct-GPT,NA,OpenAI,https://openai.com/,NA,closed,https://chat.openai.com/,OpenAI has not released any source code related to ChatGPT,closed,,OpenAI has not released or documented any of the source training data,closed,,OpenAI has not released model weights for GPT3.5,closed,,OpenAI has not released the instruction-tuning data,closed,,OpenAI has not released details about RLHF models weights,closed,,None of the code is open sourced; license unknown.,closed,,"Code is not available, documentation level unknown.",closed,,OpenAI has not clearly documented the LLM+RLHF architecture or its evaluation.,partial,https://arxiv.org/abs/2203.02155,Preprint describes only the instruction-tuning method; no further papers available.,closed,,No peer-reviewed paper documenting this system is available.,closed,https://github.com/openai/gpt-3/blob/master/model-card.md,No modelcard is available for GPT3.5. The linked model card is for GPT3 is dated Sept 2020.,closed,,OpenAI has not released a datasheet or any other documentation or evaluation of source data.,closed,,Some third party packages exist; support for them is contingent on OpenAI's whims.,closed,,API access is only available for commercial users.,/projects/chatgpt.yaml,0.5
